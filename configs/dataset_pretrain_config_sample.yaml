experiment_name: 'mica_pretrain_dataset'
phase: 'pretrain'
trial_name:

lightning:
    trainer:
       gpus: [0]
       max_epochs: 50
       min_epochs: 50
       distributed_backend: 'dp'
       gradient_clip_val: 0.25
       lr: 0.00005
       precision: 16
    checkpoint_callback:
        monitor: 'val_loss'
        dirpath: '/path/ckpt'  # NOTE: need to modify the checkpoint path
        save_last: false
        mode: min
        every_n_epochs: 10
    early_stopping_callback:
        monitor: 'val_loss'
        min_delta: 0.00
        patience: 10
        verbose: False
        mode: 'min'
    logger:
        logger_type: 'WandbLogger'
        save_dir: './out/'
        project: 'mica_pretrain'

model:
    norm: false
    mica:
        local_loss_weight: 1.0
        global_loss_weight: 1.0
        temp1: 4.0
        temp2: 5.0
        temp3: 10.0
    vision:
        model_name: 'resnet_50'
        freeze_cnn: false
        pretrained: true
    text:
        bert_type: "emilyalsentzer/Bio_ClinicalBERT"
        last_n_layers: 4
        aggregate_method: 'sum'
        norm: false
        embedding_dim: 768
        agg_tokens: true
data:
    dataset: dataset_name # NOTE: need to modify the dataset name
    text:
      word_num: 20
      captions_per_image: 5
      full_report: true
    image:
        imsize: 256

transforms:
    norm: 'half'
    random_crop:
        crop_size: 224
    random_horizontal_flip: #  0.5  
    random_affine:
#        degrees: [-10, 10]
#        translate: [0.0625, 0.0625]
#        scale: [0.8, 1.1]
    color_jitter:
#        brightness: [0.5, 1.2]
#        contrast: [0.5, 1.2]

train:
    update_interval: 1000
    batch_size: 32
    num_workers: 8
    nvis: 0
    rand_vis: false
    optimizer:
        name: 'Adam'
        weight_decay: 1e-6
    scheduler:
        name: 'plateau'
        monitor: 'val_loss'
        interval: 'epoch'
        frequency: 1
